"""
SHRI Tracker Test Suite

This test suite validates the SHRI tracker's name generation logic, specifically:
- REMUX detection for UNTOUCHED/VU releases
- Proper handling of existing REMUX releases
- Ensuring normal ENCODE releases remain unchanged
- Audio language tag insertion
- Italian title substitution

Test data can be provided via:
1. mock_test_cases.json - Predefined test cases
2. generated_meta.json - Real file metadata from meta_generator.py (default)
3. Custom file via command line argument

Usage:
    python test_SHRI.py                                 # Uses generated_meta.json
    python test_SHRI.py generated_meta_file_name.json   # Uses specific file
"""

import sys
import json
import asyncio
import os
from unittest.mock import Mock


# =============================================================================
# Mock Setup
# =============================================================================
async def mock_process_desc_language(*args, **kwargs):
    """Mock async function for language processing"""
    return None


mock_languages = Mock()
mock_languages.process_desc_language = mock_process_desc_language
sys.modules["src.languages"] = mock_languages
sys.modules["src.trackers.COMMON"] = Mock()

mock_unit3d_class = type(
    "UNIT3D", (), {"__init__": lambda self, config, tracker_name: None}
)
sys.modules["src.trackers.UNIT3D"] = Mock(UNIT3D=mock_unit3d_class)

from src.trackers.SHRI import SHRI  # noqa: E402


# =============================================================================
# Test Data Loading
# =============================================================================
def get_mock_test_cases(json_file="mock_test_cases.json"):
    """
    Load mock test cases from JSON file or return hardcoded defaults.

    Mock test cases allow testing without real media files by providing
    pre-defined metadata structures that simulate Upload Assistant's output.

    Args:
        json_file: Path to JSON file containing test cases

    Returns:
        List of test case dictionaries with 'desc', 'filename', and 'meta' keys
    """
    # Attempt to load from file
    if os.path.exists(json_file):
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                test_cases = json.load(f)
            print(f"[INFO] Loaded {len(test_cases)} mock test cases from {json_file}\n")
            return test_cases
        except Exception as e:
            print(f"[WARNING] Failed to load {json_file}: {e}")
            print("[INFO] Using hardcoded test cases instead\n")

    # Fallback to hardcoded minimal test case
    return [
        {
            "desc": "UNTOUCHED in filename",
            "filename": "Babe va in città (1998) UNTOUCHED 1080p VC-1 DTS HD ENG AC3 iTA-ENG Ciame.mkv",
            "meta": {
                "name": "Babe va in città 1998 ENCODE 1080p VC-1 DTS-HD MA ITA-ENG Ciame",
                "type": "ENCODE",
                "resolution": "1080p",
                "video_codec": "VC-1",
                "source": "BluRay",
                "audio": "DTS-HD MA",
                "year": "1998",
                "title": "Babe: Pig in the City",
                "tag": "Ciame",
            },
        }
    ]


def load_generated_meta(json_file=None, overrides=None):
    """
    Load real file metadata generated by meta_generator.py.

    This allows testing with actual Upload Assistant metadata extracted
    from real media files, ensuring test accuracy.

    Args:
        json_file: Path to generated metadata JSON file (default: generated_meta.json)
        overrides: Optional dict of values to override in loaded meta
                   Useful for testing edge cases (e.g., forcing ENCODE type)

    Returns:
        Test case dictionary, or None if file doesn't exist
    """
    if json_file is None:
        json_file = "generated_meta.json"

    if not os.path.exists(json_file):
        return None

    with open(json_file, "r", encoding="utf-8") as f:
        real_meta = json.load(f)

    # Apply overrides for testing specific scenarios
    if overrides:
        real_meta.update(overrides)
        print(f"[INFO] Applied overrides: {list(overrides.keys())}")

    if "filelist" in real_meta and real_meta["filelist"]:
        filename = os.path.basename(real_meta["filelist"][0])
    else:
        filename = os.path.basename(real_meta.get("path", "unknown.mkv"))

    return {
        "desc": f"Generated meta from {os.path.basename(json_file)}",
        "filename": filename,
        "meta": real_meta,
    }


# =============================================================================
# Test Execution
# =============================================================================
async def run_test(test_case, shri):
    """
    Execute a single test case and validate results.

    Args:
        test_case: Dict containing 'desc', 'filename', and 'meta'
        shri: SHRI tracker instance

    Returns:
        bool: True if test passed, False otherwise
    """
    meta = test_case["meta"].copy()

    if "name" not in meta:
        meta["name"] = test_case["filename"].replace(".mkv", "").replace(".", " ")

    defaults = {
        "filelist": [test_case["filename"]],
        "path": test_case["filename"],
        "video_encode": "x264",
        "is_disc": False,
        "language_checked": True,
        "audio_languages": ["ENG"],
        "dual_audio": False,
    }

    for key, value in defaults.items():
        if key not in meta:
            meta[key] = value

    result = await shri.get_name(meta)

    print(f"Test: {test_case['desc']}")
    print(f"File: {test_case['filename']}")
    print(f"In:   {meta['name']}")
    print(f"Out:  {result['name']}")

    has_untouched_or_vu = (
        "untouched" in test_case["filename"].lower()
        or "vu" in test_case["filename"].lower()
    )
    should_be_remux = has_untouched_or_vu or meta["type"] == "REMUX"

    passed = True
    if should_be_remux:
        if "REMUX" in result["name"]:
            print("✓ Correctly identified as REMUX")
        else:
            print("✗ FAILED: Should be REMUX but isn't")
            passed = False
    else:
        if "REMUX" in result["name"]:
            print("✗ FAILED: Should stay ENCODE but became REMUX")
            passed = False
        else:
            print("✓ Correctly stayed ENCODE")

    print()
    return passed


async def test_remux_detection():
    """
    Main test runner for SHRI tracker REMUX detection logic.

    Collects test cases from multiple sources and executes them,
    providing a summary of results.
    """
    config = {"TRACKERS": {"SHRI": {"use_italian_title": False}}}
    shri = SHRI(config)

    test_cases = []
    test_cases.extend(get_mock_test_cases())

    # ==========================================================================
    # OVERRIDE SECTION
    # ==========================================================================
    meta_overrides = None

    # Example overrides for testing:
    # meta_overrides = {
    #     'type': 'ENCODE',           # Force ENCODE to test REMUX detection
    #     'audio_languages': ['ITA'], # Override detected audio languages
    # }
    # ==========================================================================

    # Check for command-line argument specifying meta file
    meta_file = sys.argv[1] if len(sys.argv) > 1 else None

    generated = load_generated_meta(json_file=meta_file, overrides=meta_overrides)
    if generated:
        test_cases.append(generated)
        file_used = meta_file or "generated_meta.json"
        print(f"[INFO] Loaded generated meta from {file_used}\n")
    else:
        if meta_file:
            print(f"[WARNING] File not found: {meta_file}\n")
        print("[INFO] Running with mock data only\n")

    print("=" * 60)
    print("REMUX DETECTION TESTS")
    print("=" * 60)
    print()

    results = []
    for test_case in test_cases:
        passed = await run_test(test_case, shri)
        results.append(passed)

    print("=" * 60)
    passed_count = sum(results)
    total_count = len(results)
    print(f"Results: {passed_count}/{total_count} tests passed")

    if passed_count == total_count:
        print("✓ All tests passed!")
    else:
        print(f"✗ {total_count - passed_count} test(s) failed")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(test_remux_detection())
